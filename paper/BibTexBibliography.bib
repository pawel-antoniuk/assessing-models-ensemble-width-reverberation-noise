
@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-06-22},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	keywords = {\_tablet},
	pages = {84--90},
	file = {ImageNet classification with deep convolutional neural networksKrizhevsky et al_2017_.pdf:/Users/pawel/Zotero/storage/LRKNMTBQ/ImageNet classification with deep convolutional neural networksKrizhevsky et al_2017_.pdf:application/pdf},
}

@inproceedings{noauthor_itu-r_2023,
	address = {Geneva, Switzerland},
	title = {{ITU}-{R} {BS}.1770-5: {Algorithms} to measure audio programme loudness and true-peak audio level},
	copyright = {International Communications Union},
	language = {en},
	booktitle = {International {Communications} {Union}},
	month = nov,
	year = {2023},
	file = {Recommendation ITU-R BS.1770-5 (112023) Algorithm.pdf:/Users/pawel/Zotero/storage/KP326E9A/Recommendation ITU-R BS.1770-5 (112023) Algorithm.pdf:application/pdf},
}

@article{hahmann_sound_2022,
	title = {Sound source localization using multiple ad hoc distributed microphone arrays},
	volume = {2},
	issn = {2691-1191},
	doi = {10.1121/10.0011811},
	abstract = {Sound source localization is crucial for communication and sound scene analysis. This study uses direction-of-arrival estimates of multiple ad hoc distributed microphone arrays to localize sound sources in a room. An affine mapping between the independent array estimates and the source coordinates is derived from a set of calibration points. Experiments show that the affine model is sufficient to locate a source and can be calibrated to physical dimensions. A projection of the local array estimates increases localization accuracy, particularly further away from the calibrated region. Localization tests in three dimensions compare the affine approach to a nonlinear neural network.},
	language = {eng},
	number = {7},
	journal = {JASA Express Letters},
	author = {Hahmann, Manuel and Fernandez-Grande, Efren and Gunawan, Henrry and Gerstoft, Peter},
	month = jul,
	year = {2022},
	pmid = {36154052},
	keywords = {Acoustics, Sound, Sound Localization},
	pages = {074801},
	file = {Full Text:/Users/pawel/Zotero/storage/5MD884GP/Hahmann et al. - 2022 - Sound source localization using multiple ad hoc di.pdf:application/pdf},
}

@article{king_how_2001,
	title = {How {Plastic} {Is} {Spatial} {Hearing}?},
	volume = {6},
	issn = {1420-3030},
	url = {https://doi.org/10.1159/000046829},
	doi = {10.1159/000046829},
	abstract = {The location of a sound source is derived by the auditory system from spatial cues present in the signals at the two ears. These cues include interaural timing and level differences, as well as monaural spectral cues generated by the external ear. The values of these cues vary with individual differences in the shape and dimensions of the head and external ears. We have examined the neurophysiological consequences of these intersubject variations by recording the responses of neurons in ferret primary auditory cortex to virtual sound sources mimicking the animal’s own ears or those of other ferrets. For most neurons, the structure of the spatial response fields changed significantly when acoustic cues measured from another animal were presented. This is consistent with the finding that humans localize less accurately when listening to virtual sounds from other subjects. To examine the role of experience in shaping the ability to localize sound, we have studied the behavioural consequences of altering binaural cues by chronically plugging one ear. Ferrets raised and tested with one ear plugged learned to localize as accurately as control animals, which is consistent with previous findings that the representation of auditory space in the midbrain can accommodate abnormal sensory cues during development. Adaptive changes in behaviour were also observed in adults, particularly if they were provided with regular practice in the localization task. Together, these findings suggest that the neural circuits responsible for sound localization can be recalibrated throughout life.},
	number = {4},
	urldate = {2024-06-24},
	journal = {Audiology and Neurotology},
	author = {King, Andrew J. and Kacelnik, Oliver and Mrsic-Flogel, Thomas D. and Schnupp, Jan W.H. and Parsons, Carl H. and Moore, David R.},
	month = nov,
	year = {2001},
	pages = {182--186},
}

@article{lin_network_2013,
	title = {Network {In} {Network}},
	volume = {abs/1312.4400},
	url = {https://api.semanticscholar.org/CorpusID:16636683},
	journal = {CoRR},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	year = {2013},
	file = {arXiv Fulltext PDF:/Users/pawel/Zotero/storage/3FFXZK4D/Lin et al. - 2014 - Network In Network.pdf:application/pdf},
}

@article{arthi_spatiogram_2021,
	title = {Spatiogram: {A} phase based directional angular measure and perceptual weighting for ensemble source width},
	volume = {abs/2112.07216},
	url = {https://arxiv.org/abs/2112.07216},
	journal = {ArXiv},
	author = {Arthi, S. and Sreenivas, Thippur V.},
	year = {2021},
	file = {arXiv Fulltext PDF:/Users/pawel/Zotero/storage/IRU2Y2LJ/S and T V - 2021 - Spatiogram A phase based directional angular meas.pdf:application/pdf},
}

@inproceedings{pavlidi_real-time_2012,
	title = {Real-time multiple sound source localization using a circular microphone array based on single-source confidence measures},
	url = {https://ieeexplore.ieee.org/document/6288455},
	doi = {10.1109/ICASSP.2012.6288455},
	abstract = {We propose a novel real-time adaptative localization approach for multiple sources using a circular array, in order to suppress the localization ambiguities faced with linear arrays, and assuming a weak sound source sparsity which is derived from blind source separation methods. Our proposed method performs very well both in simulations and in real conditions at 50\% real-time.},
	urldate = {2024-06-07},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Pavlidi, Despoina and Puigt, Matthieu and Griffin, Anthony and Mouchtaris, Athanasios},
	month = mar,
	year = {2012},
	note = {ISSN: 2379-190X},
	keywords = {Array signal processing, Arrays, direction of arrival estimation, Direction of arrival estimation, Estimation, Microphones, multiple source localization, Real time systems, Speech, Time frequency analysis},
	pages = {2625--2628},
	file = {Full Text:/Users/pawel/Zotero/storage/FXAAV3EW/Pavlidi et al. - 2012 - Real-time multiple sound source localization using.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/RI3ZVKSY/6288455.html:text/html},
}

@inproceedings{antoniuk_blind_2023,
	title = {Blind estimation of ensemble width in binaural music recordings using ‘spatiograms’ under simulated anechoic conditions},
	copyright = {All rights reserved},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=22203},
	booktitle = {Audio {Engineering} {Society} {Conference}: {AES} 2023 {International} {Conference} on {Spatial} and {Immersive} {Audio}},
	author = {Antoniuk, Paweł and Zieliński, Sławomir K.},
	month = aug,
	year = {2023},
	file = {Antoniuk and Zieliński - 2023 - Blind estimation of ensemble width in binaural mus.pdf:/Users/pawel/Zotero/storage/SJ27GPEU/Antoniuk and Zieliński - 2023 - Blind estimation of ensemble width in binaural mus.pdf:application/pdf},
}

@article{yang_deepear_2024,
	title = {{DeepEar}: {Sound} {Localization} {With} {Binaural} {Microphones}},
	volume = {23},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1536-1233, 1558-0660, 2161-9875},
	shorttitle = {{DeepEar}},
	url = {https://ieeexplore.ieee.org/document/9954178/},
	doi = {10.1109/TMC.2022.3222821},
	abstract = {The binaural microphone, which refers to a pair of microphones with artificial human-shaped ears, is widely used in hearing aids and spatial audio recording to improve sound quality. It is crucial for such devices to find the voice direction in many applications such as binaural sound enhancement. However, sound localization with two microphones remains challenging, especially in multi-source scenarios. Most previous work utilized microphone arrays to deal with the multi-source localization problem. Extra microphones yet have space constraints for deployment in many scenarios (e.g., hearing aids). Inspired by the fact that humans have evolved to locate multiple sound sources with only two ears, we propose DeepEar, a binaural microphone-based sound localization system. To this end, we design a multisector-based neural network to locate multiple sound sources simultaneously, where each sector is a discretized region of the space for different angle of arrivals. DeepEar fuses explicit hand-crafted features and implicit latent sound representatives to facilitate sound localization. More importantly, the trained DeepEar model can adapt to new environments with a minimum amount of extra training data. The experiment results show that DeepEar substantially outperforms the state-of-the-art binaural deep learning approach by a large margin in terms of sound detection accuracy and azimuth estimation error.},
	language = {en},
	number = {1},
	urldate = {2024-06-10},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Yang, Qiang and Zheng, Yuanqing},
	month = jan,
	year = {2024},
	pages = {359--375},
	file = {Yang and Zheng - 2024 - DeepEar Sound Localization With Binaural Micropho.pdf:/Users/pawel/Zotero/storage/NILP5UKH/Yang and Zheng - 2024 - DeepEar Sound Localization With Binaural Micropho.pdf:application/pdf},
}

@article{liu_sound_2022,
	title = {Sound {Source} {Localization} {Based} on {Multi}-{Channel} {Cross}-{Correlation} {Weighted} {Beamforming}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-666X},
	url = {https://www.mdpi.com/2072-666X/13/7/1010},
	doi = {10.3390/mi13071010},
	abstract = {Beamforming and its applications in steered-response power (SRP) technology, such as steered-response power delay and sum (SRP-DAS) and steered-response power phase transform (SRP-PHAT), are widely used in sound source localization. However, their resolution and accuracy still need improvement. A novel beamforming method combining SRP and multi-channel cross-correlation coefficient (MCCC), SRP-MCCC, is proposed in this paper to improve the accuracy of direction of arrival (DOA). Directional weight (DW) is obtained by calculating the MCCC. Based on DW, suppressed the non-incoming wave direction and gained the incoming wave direction to improve the beamforming capabilities. Then, sound source localizations based on the dual linear array under different conditions were simulated. Compared with SRP-PHAT, SRP-MCCC has the advantages of high positioning accuracy, strong spatial directivity and robustness under the different signal–noise ratios (SNRs). When the SNR is −10 dB, the average positioning error of the single-frequency sound source at different coordinates decreases by 5.69\%, and that of the mixed frequency sound sources at the same coordinate decreases by 5.77\%. Finally, the experimental verification was carried out. The results show that the average error of SRP-MCCC has been reduced by 8.14\% and the positioning accuracy has been significantly improved, which is consistent with the simulation results. This research provides a new idea for further engineering applications of sound source localization based on beamforming.},
	language = {en},
	number = {7},
	urldate = {2024-06-07},
	journal = {Micromachines},
	author = {Liu, Mengran and Hu, Junhao and Zeng, Qiang and Jian, Zeming and Nie, Lei},
	month = jul,
	year = {2022},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {beamforming, microphone array, multi-channel cross-correlation coefficient, sound source localization},
	pages = {1010},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/J597NXKN/Liu et al. - 2022 - Sound Source Localization Based on Multi-Channel C.pdf:application/pdf},
}

@article{chung_sound_2022,
	title = {Sound {Localization} {Based} on {Acoustic} {Source} {Using} {Multiple} {Microphone} {Array} in an {Indoor} {Environment}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/6/890},
	doi = {10.3390/electronics11060890},
	abstract = {Sound signals have been widely applied in various fields. One of the popular applications is sound localization, where the location and direction of a sound source are determined by analyzing the sound signal. In this study, two microphone linear arrays were used to locate the sound source in an indoor environment. The TDOA is also designed to deal with the problem of delay in the reception of sound signals from two microphone arrays by using the generalized cross-correlation algorithm to calculate the TDOA. The proposed microphone array system with the algorithm can successfully estimate the sound source’s location. The test was performed in a standardized chamber. This experiment used two microphone arrays, each with two microphones. The experimental results prove that the proposed method can detect the sound source and obtain good performance with a position error of about 2.0{\textasciitilde}2.3 cm and angle error of about 0.74 degrees. Therefore, the experimental results demonstrate the feasibility of the system.},
	language = {en},
	number = {6},
	urldate = {2024-06-07},
	journal = {Electronics},
	author = {Chung, Ming-An and Chou, Hung-Chi and Lin, Chia-Wei},
	month = jan,
	year = {2022},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {generalized cross-correlation algorithm, indoor localization, microphone array, sound localization, time difference of arrival},
	pages = {890},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/PX2QWICQ/Chung et al. - 2022 - Sound Localization Based on Acoustic Source Using .pdf:application/pdf},
}

@article{pan_multi-tone_2021,
	title = {Multi-{Tone} {Phase} {Coding} of {Interaural} {Time} {Difference} for {Sound} {Source} {Localization} {With} {Spiking} {Neural} {Networks}},
	volume = {29},
	issn = {2329-9304},
	url = {https://ieeexplore.ieee.org/document/9502013},
	doi = {10.1109/TASLP.2021.3100684},
	abstract = {Mammals exhibit remarkable capability of detecting and localizing sound sources in complex acoustic environments by using binaural cues in the spiking manner. Emulating the auditory process for sound source localization (SSL) by mammals, we propose a computational model for accurate and robust SSL under the neuromorphic spiking neural network (SNN) framework. The center of this model is a Multi-Tone Phase Coding (MTPC) scheme, which encodes the interaural time difference (ITD) between binaural pure tones into discriminative spike patterns that can be directly classified by SNNs. As such, SSL can be implemented as an event-driven task on highly efficient, neuromorphic parallel processors. We evaluate the proposed computational model on a directional audio dataset recorded from a microphone array in a realistic acoustic environment with background noise, obstruction, reflection, and other interferences. We report superior localization capability with a mean absolute error (MAE) of 1.02° or 100\% classification accuracy with an angle resolution of 5°, which surpasses other SNN-based biologically plausible neuromorphic approaches by a relatively large margin and on par with human performance in similar tasks. This study opens up many application opportunities in human-robot interaction where energy efficiency is crucial. As a case study, we successfully deploy the proposed SSL system in a robotic platform to track the speaker and orient the robot's attention.},
	urldate = {2024-06-07},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Pan, Zihan and Zhang, Malu and Wu, Jibin and Wang, Jiadong and Li, Haizhou},
	year = {2021},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {Acoustics, Biological neural networks, Computational modeling, Ear, Encoding, Location awareness, Neural phase coding, Neurons, sound source localization, spiking neural network},
	pages = {2656--2670},
	file = {IEEE Xplore Abstract Record:/Users/pawel/Zotero/storage/8CJB3YFJ/9502013.html:text/html},
}

@incollection{bregman_auditory_1990,
	title = {Auditory {Scene} {Analysis}: {The} {Perceptual} {Organization} of {Sound}},
	volume = {95},
	shorttitle = {Auditory {Scene} {Analysis}},
	abstract = {Scitation is the online home of leading journals and conference proceedings from AIP Publishing and AIP Member Societies},
	booktitle = {Journal of {The} {Acoustical} {Society} of {America} - {J} {ACOUST} {SOC} {AMER}},
	author = {Bregman, Albert},
	month = jan,
	year = {1990},
	doi = {10.1121/1.408434},
	note = {Journal Abbreviation: Journal of The Acoustical Society of America - J ACOUST SOC AMER},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/SR53ZIS7/Bregman - 1990 - Auditory Scene Analysis The Perceptual Organizati.pdf:application/pdf},
}

@article{ma_exploiting_2017,
	title = {Exploiting {Deep} {Neural} {Networks} and {Head} {Movements} for {Robust} {Binaural} {Localization} of {Multiple} {Sources} in {Reverberant} {Environments}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8086216/},
	doi = {10.1109/TASLP.2017.2750760},
	abstract = {This paper presents a novel machine-hearing system that exploits deep neural networks (DNNs) and head movements for robust binaural localization of multiple sources in reverberant environments. DNNs are used to learn the relationship between the source azimuth and binaural cues, consisting of the complete cross-correlation function (CCF) and interaural level differences (ILDs). In contrast to many previous binaural hearing systems, the proposed approach is not restricted to localization of sound sources in the frontal hemiﬁeld. Due to the similarity of binaural cues in the frontal and rear hemiﬁelds, front–back confusions often occur. To address this, a head movement strategy is incorporated in the localization model to help reduce the front–back errors. The proposed DNN system is compared to a Gaussian-mixture-model-based system that employs interaural time differences (ITDs) and ILDs as localization features. Our experiments show that the DNN is able to exploit information in the CCF that is not available in the ITD cue, which together with head movements substantially improves localization accuracies under challenging acoustic scenarios, in which multiple talkers and room reverberation are present.},
	language = {en},
	number = {12},
	urldate = {2021-05-01},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ma, Ning and May, Tobias and Brown, Guy J.},
	month = dec,
	year = {2017},
	keywords = {\_tablet},
	pages = {2444--2453},
	annote = {Extracted Annotations (11/19/2021, 1:27:55 AM)"To address this, a head movement strategy is incorporated in the localization model to help reduce the front-back errors." (Ma et al 2017:2444)"[3]-[6]" (Ma et al 2017:2444)"bank of cochlear filters" (Ma et al 2017:2444)"Within each frequency band, a DNN takes as input features the cross-correlation function (CCF) (as opposed to a single estimate of ITD) and the ILD" (Ma et al 2017:2445)"presents a number of source localisation experiments, in which head movements are simulated by using binaural room impulse responses (BRIRs) to generate direction-dependent binaural sound mixtures" (Ma et al 2017:2445)"A separate DNN was trained for each of the 32 frequency bands. Employing frequency-dependent DNNs was found to be effective for localising simultaneous sound sources. Although simultaneous sources overlap in time, within a local time frame each frequency band is mostly dominated by a single source (Bregman's [19] notion of 'exclusive allocation'). Hence, this allows training using single-source data and removes the need to include multi-source data for training." (Ma et al 2017:2445)"C. Localisation With Head Movements" (Ma et al 2017:2446)"based on the Knowles Electronic Manikin for Acoustic Research (KEMAR) head and torso simulator with pinnae [20]" (Ma et al 2017:2447)"Surrey BRIR database" (Ma et al 2017:2447)"Surrey HATS HRIRs catalog is only available for the frontal azimuth angles and therefore cannot ◦ be used to train the full 360 localisation models" (Ma et al 2017:2447)"GMM framework" (Ma et al 2017:2448)},
	file = {Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/V7F5UFEG/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/GPKBUAUB/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/LUE4X72E/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/CUKBVRBR/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/8CJTDBAU/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/J3TLLSRH/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/8VWGWJNY/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/ZULSSA36/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/A34GTMYI/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf;Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:/Users/pawel/Zotero/storage/WJ6SRS8A/Exploiting Deep Neural Networks and Head Movements for Robust BinauralMa et al_2017_.pdf:application/pdf},
}

@article{benaroya_binaural_2018,
	title = {Binaural {Localization} of {Multiple} {Sound} {Sources} by {Non}-{Negative} {Tensor} {Factorization}},
	volume = {26},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/8294267/},
	doi = {10.1109/TASLP.2018.2806745},
	abstract = {This paper presents non-negative factorization of audio signals for the binaural localization of multiple sound sources within realistic and unknown sound environments. Nonnegative tensor factorization (NTF) provides a sparse representation of multi-channel audio signals in time, frequency, and space that can be exploited in computational audio scene analysis and robot audition for the separation and localization of sound sources. In the proposed formulation, each sound source is represented by mean of spectral dictionaries, temporal activation, and its distribution within each channel (here, left and right ears). This distribution, being dependent on the frequency, can be interpreted as an explicit estimation of the Head-Related Transfer Function (HRTF) of a binaural head which can then be converted into the estimated sound source position. Moreover, the semi-supervised formulation of the non-negative factorization allows to integrate prior knowledge about some sound sources of interest whose dictionaries can be learned in advance, whereas the remaining sources are considered as background sound which remains unknown and is estimated on-the-ﬂy. The proposed NTFbased sound source localization is here applied to binaural sound source localization of multiple speakers within realistic sound environments.},
	language = {en},
	number = {6},
	urldate = {2021-05-01},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Benaroya, Elie Laurent and Obin, Nicolas and Liuni, Marco and Roebel, Axel and Raumel, Wilson and Argentieri, Sylvain},
	month = jun,
	year = {2018},
	keywords = {\_tablet},
	pages = {1072--1082},
	file = {Binaural Localization of Multiple Sound Sources by Non-Negative TensorBenaroya et al_2018_.pdf:/Users/pawel/Zotero/storage/9MF3WI4W/Binaural Localization of Multiple Sound Sources by Non-Negative TensorBenaroya et al_2018_.pdf:application/pdf},
}

@article{madole_3-d_1995,
	title = {3-{D} {Sound} for {Virtual} {Reality} and {Multimedia}},
	volume = {19},
	issn = {01489267},
	url = {https://www.jstor.org/stable/3680997?origin=crossref},
	doi = {10.2307/3680997},
	language = {en},
	number = {4},
	urldate = {2021-05-01},
	journal = {Computer Music Journal},
	author = {Madole, Dave and Begault, Durand},
	year = {1995},
	keywords = {\_tablet, binaural, spatial audio},
	pages = {99},
	annote = {notatka},
	file = {3-D Sound for Virtual Reality and MultimediaMadole_Begault_1995_.pdf:/Users/pawel/Zotero/storage/683D5VAD/3-D Sound for Virtual Reality and MultimediaMadole_Begault_1995_.pdf:application/pdf},
}

@inproceedings{ma_speech_2016,
	title = {Speech {Localisation} in a {Multitalker} {Mixture} by {Humans} and {Machines}},
	url = {https://www.isca-speech.org/archive/interspeech_2016/ma16c_interspeech.html},
	doi = {10.21437/Interspeech.2016-1149},
	language = {en},
	urldate = {2022-01-12},
	booktitle = {Interspeech 2016},
	publisher = {ISCA},
	author = {Ma, Ning and Brown, Guy J.},
	month = sep,
	year = {2016},
	keywords = {\_tablet},
	pages = {3359--3363},
	file = {Speech Localisation in a Multitalker Mixture by Humans and MachinesMa_Brown_2016_.pdf:/Users/pawel/Zotero/storage/EFUP7AJ9/Speech Localisation in a Multitalker Mixture by Humans and MachinesMa_Brown_2016_.pdf:application/pdf},
}

@article{rumsey_spatial_2002,
	title = {Spatial {Quality} {Evaluation} for {Reproduced} {Sound}: {Terminology}, {Meaning}, and a {Scene}-{Based} {Paradigm}},
	volume = {50},
	shorttitle = {Spatial {Quality} {Evaluation} for {Reproduced} {Sound}},
	abstract = {Spatial quality in reproduced sound is a subset of the broad topic of sound quality. In the past it has been studied less rigorously than other aspects of reproduced sound quality, leading to a lack of clarity in standard definitions of subjective attributes. Rigor in the physical measurement of sound signals should be matched by equal rigor in semantics relating to subjective evaluation. A scene-based paradigm for the description and assessment of spatial quality is described, which enables clear distinctions to be made between elements of a reproduced sound scene and will assist in the search for related physical parameters.},
	journal = {Journal of the Audio Engineering Society},
	author = {Rumsey, Francis},
	month = sep,
	year = {2002},
	pages = {651--666},
}

@article{cherry_experiments_1953,
	title = {Some {Experiments} on the {Recognition} of {Speech}, with {One} and with {Two} {Ears}},
	volume = {25},
	issn = {0001-4966},
	url = {https://doi.org/10.1121/1.1907229},
	doi = {10.1121/1.1907229},
	abstract = {This paper describes a number of objective experiments on recognition, concerning particularly the relation between the messages received by the two ears. Rather than use steady tones or clicks (frequency or time‐point signals) continuous speech is used, and the results interpreted in the main statistically.Two types of test are reported: (a) the behavior of a listener when presented with two speech signals simultaneously (statistical filtering problem) and (b) behavior when different speech signals are presented to his two ears.},
	number = {5},
	journal = {The Journal of the Acoustical Society of America},
	author = {Cherry, E. Colin},
	month = sep,
	year = {1953},
	note = {\_eprint: https://pubs.aip.org/asa/jasa/article-pdf/25/5/975/18731769/975\_1\_online.pdf},
	pages = {975--979},
}

@inproceedings{may_robust_2015,
	address = {South Brisbane, Queensland, Australia},
	title = {Robust localisation of multiple speakers exploiting head movements and multi-conditional training of binaural cues},
	isbn = {978-1-4673-6997-8},
	url = {http://ieeexplore.ieee.org/document/7178457/},
	doi = {10.1109/ICASSP.2015.7178457},
	abstract = {This paper addresses the problem of localising multiple competing speakers in the presence of room reverberation, where sound sources can be positioned at any azimuth on the horizontal plane. To reduce the amount of front-back confusions which can occur due to the similarity of interaural time differences (ITDs) and interaural level differences (ILDs) in the front and rear hemiﬁeld, a machine hearing system is presented which combines supervised learning of binaural cues using multi-conditional training (MCT) with a head movement strategy. A systematic evaluation showed that this approach substantially reduced the amount of front-back confusions in challenging acoustic scenarios. Moreover, the system was able to generalise to a variety of different acoustic conditions not seen during training.},
	language = {en},
	urldate = {2024-05-08},
	booktitle = {2015 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {May, Tobias and Ma, Ning and Brown, Guy J.},
	month = apr,
	year = {2015},
	pages = {2679--2683},
	file = {May et al. - 2015 - Robust localisation of multiple speakers exploitin.pdf:/Users/pawel/Zotero/storage/ZVTKTXZK/May et al. - 2015 - Robust localisation of multiple speakers exploitin.pdf:application/pdf},
}

@article{woodruff_binaural_2012,
	title = {Binaural {Localization} of {Multiple} {Sources} in {Reverberant} and {Noisy} {Environments}},
	volume = {20},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6129395/},
	doi = {10.1109/TASL.2012.2183869},
	abstract = {Sound source localization from a binaural input is a challenging problem, particularly when multiple sources are active simultaneously and reverberation or background noise are present. In this work, we investigate a multi-source localization framework in which monaural source segregation is used as a mechanism to increase the robustness of azimuth estimates from a binaural input. We demonstrate performance improvement relative to binaural only methods assuming a known number of spatially stationary sources. We also propose a ﬂexible azimuth-dependent model of binaural features that independently captures characteristics of the binaural setup and environmental conditions, allowing for adaptation to new environments or calibration to an unseen binaural setup. Results with both simulated and recorded impulse responses show that robust performance can be achieved with limited prior training.},
	language = {en},
	number = {5},
	urldate = {2024-05-08},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Woodruff, John and Wang, DeLiang},
	month = jul,
	year = {2012},
	pages = {1503--1512},
	file = {Woodruff and Wang - 2012 - Binaural Localization of Multiple Sources in Rever.pdf:/Users/pawel/Zotero/storage/FJTJSIIN/Woodruff and Wang - 2012 - Binaural Localization of Multiple Sources in Rever.pdf:application/pdf},
}

@article{may_binaural_2012,
	title = {A {Binaural} {Scene} {Analyzer} for {Joint} {Localization} and {Recognition} of {Speakers} in the {Presence} of {Interfering} {Noise} {Sources} and {Reverberation}},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/6178270/},
	doi = {10.1109/TASL.2012.2193391},
	abstract = {In this study, we present a binaural scene analyzer that is able to simultaneously localize, detect and identify a known number of target speakers in the presence of spatially positioned noise sources and reverberation. In contrast to many other binaural cocktail party processors, the proposed system does not require a priori knowledge about the azimuth position of the target speakers. The proposed system consists of three main building blocks: binaural localization, speech source detection, and automatic speaker identiﬁcation. First, a binaural front-end is used to robustly localize relevant sound source activity. Second, a speech detection module based on missing data classiﬁcation is employed to determine whether detected sound source activity corresponds to a speaker or to an interfering noise source using a binary mask that is based on spatial evidence supplied by the binaural front-end. Third, a second missing data classiﬁer is used to recognize the speaker identities of all detected speech sources. The proposed system is systematically evaluated in simulated adverse acoustic scenarios. Compared to state-of-the art MFCC recognizers, the proposed model achieves signiﬁcant speaker recognition accuracy improvements.},
	language = {en},
	number = {7},
	urldate = {2024-05-08},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {May, Tobias and Van De Par, Steven and Kohlrausch, Armin},
	month = sep,
	year = {2012},
	pages = {2016--2030},
	file = {May et al. - 2012 - A Binaural Scene Analyzer for Joint Localization a.pdf:/Users/pawel/Zotero/storage/T4TC66EC/May et al. - 2012 - A Binaural Scene Analyzer for Joint Localization a.pdf:application/pdf},
}

@article{dietz_auditory_2011,
	title = {Auditory model based direction estimation of concurrent speakers from binaural signals},
	volume = {53},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016763931000097X},
	doi = {10.1016/j.specom.2010.05.006},
	abstract = {Humans show a very robust ability to localize sounds in adverse conditions. Computational models of binaural sound localization and technical approaches of direction-of-arrival (DOA) estimation also show good performance, however, both their binaural feature extraction and the strategies for further analysis partly diﬀer from what is currently known about the human auditory system. This study investigates auditory model based DOA estimation emphasizing known features and limitations of the auditory binaural processing such as (i) high temporal resolution, (ii) restricted frequency range to exploit temporal ﬁne-structure, (iii) use of temporal envelope disparities, and (iv) a limited range to compensate for interaural time delay. DOA estimation performance was investigated for up to ﬁve concurrent speakers in free ﬁeld and for up to three speakers in the presence of noise. The DOA errors in these conditions were always smaller than 5°. A condition with moving speakers was also tested and up to three moving speakers could be tracked simultaneously. Analysis of DOA performance as a function of the binaural temporal resolution showed that short time constants of about 5 ms employed by the auditory model were crucial for robustness against concurrent sources.},
	language = {en},
	number = {5},
	urldate = {2023-06-04},
	journal = {Speech Communication},
	author = {Dietz, Mathias and Ewert, Stephan D. and Hohmann, Volker},
	month = may,
	year = {2011},
	pages = {592--605},
	file = {Dietz et al. - 2011 - Auditory model based direction estimation of concu.pdf:/Users/pawel/Zotero/storage/N82DLETS/Dietz et al. - 2011 - Auditory model based direction estimation of concu.pdf:application/pdf},
}

@article{may_probabilistic_2011,
	title = {A {Probabilistic} {Model} for {Robust} {Localization} {Based} on a {Binaural} {Auditory} {Front}-{End}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5406118/},
	doi = {10.1109/TASL.2010.2042128},
	abstract = {Although extensive research has been done in the ﬁeld of machine-based localization, the degrading effect of reverberation and the presence of multiple sources on localization performance has remained a major problem. Motivated by the ability of the human auditory system to robustly analyze complex acoustic scenes, the associated peripheral stage is used in this paper as a front-end to estimate the azimuth of sound sources based on binaural signals. One classical approach to localize an acoustic source in the horizontal plane is to estimate the interaural time difference (ITD) between both ears by searching for the maximum in the cross-correlation function. Apart from ITDs, the interaural level difference (ILD) can contribute to localization, especially at higher frequencies where the wavelength becomes smaller than the diameter of the head, leading to ambiguous ITD information. The interdependency of ITD and ILD on azimuth is a complex pattern that depends also on the room acoustics, and is therefore learned by azimuth-dependent Gaussian mixture models (GMMs). Multiconditional training is performed to take into account the variability of the binaural features which results from multiple sources and the effect of reverberation. The proposed localization model outperforms state-of-the-art localization techniques in simulated adverse acoustic conditions.},
	language = {en},
	number = {1},
	urldate = {2023-06-04},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {May, Tobias and Van De Par, Steven and Kohlrausch, Armin},
	month = jan,
	year = {2011},
	pages = {1--13},
	file = {May et al. - 2011 - A Probabilistic Model for Robust Localization Base.pdf:/Users/pawel/Zotero/storage/JU6C46FG/May et al. - 2011 - A Probabilistic Model for Robust Localization Base.pdf:application/pdf},
}

@inproceedings{arthi_binaural_2022,
	address = {Bangalore, India},
	title = {Binaural {Spatial} {Transform} for {Multi}-source {Localization} determining {Angular} {Extent} of {Ensemble} {Source} {Width}},
	isbn = {978-1-66548-250-9},
	url = {https://ieeexplore.ieee.org/document/9840782/},
	doi = {10.1109/SPCOM55316.2022.9840782},
	abstract = {In the case of ensemble like distributed presentation, inter-aural cross-correlation (IACC) is correlated with the measure for source width extension. We question the validity of this measure and develop an angular measure for ensemble source width. In this work, we distinguish the binaural correlation functions of localized source, source with reverberation and ensemble source. We also develop a novel phase-only spatial transform to localize as many sources as possible. The angular separation between the spatial extrema sources can be a physical measure for ensemble source width. We also observe that phaseonly HRIR cross-correlation functions act as listener dependent functional bases for localizing multiple sources. We observe these functional bases are wavelet-like and their signature are listener dependent and direction dependent. We extend the spatial transform to time-varying short-time spatial transform and deﬁne “Spatio-gram” to understand the effect of time-varying nature of the signal.},
	language = {en},
	urldate = {2023-02-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Signal} {Processing} and {Communications} ({SPCOM})},
	publisher = {IEEE},
	author = {Arthi, S and Sreenivas, T V},
	month = jul,
	year = {2022},
	pages = {1--5},
	file = {Arthi and Sreenivas - 2022 - Binaural Spatial Transform for Multi-source Locali.pdf:/Users/pawel/Zotero/storage/2KFT7B9W/Arthi and Sreenivas - 2022 - Binaural Spatial Transform for Multi-source Locali.pdf:application/pdf},
}

@misc{simonetta_multimodal_2019,
	title = {Multimodal music information processing and retrieval: survey and future challenges},
	shorttitle = {Multimodal music information processing and retrieval},
	url = {http://arxiv.org/abs/1902.05347},
	doi = {10.48550/arXiv.1902.05347},
	abstract = {Towards improving the performance in various music information processing tasks, recent studies exploit different modalities able to capture diverse aspects of music. Such modalities include audio recordings, symbolic music scores, mid-level representations, motion, and gestural data, video recordings, editorial or cultural tags, lyrics and album cover arts. This paper critically reviews the various approaches adopted in Music Information Processing and Retrieval and highlights how multimodal algorithms can help Music Computing applications. First, we categorize the related literature based on the application they address. Subsequently, we analyze existing information fusion approaches, and we conclude with the set of challenges that Music Information Retrieval and Sound and Music Computing research communities should focus in the next years.},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Simonetta, Federico and Ntalampiras, Stavros and Avanzini, Federico},
	month = feb,
	year = {2019},
	note = {arXiv:1902.05347},
	keywords = {Computer Science - Information Retrieval, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/pawel/Zotero/storage/CG2HDS6E/Simonetta et al. - 2019 - Multimodal music information processing and retrieval survey and future challenges.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/AHUBJ78V/1902.html:text/html},
}

@article{antoniuk_estimating_2024,
	title = {Estimating {Ensemble} {Location} and {Width} in {Binaural} {Recordings} of {Music} with {Convolutional} {Neural} {Networks}},
	copyright = {All rights reserved},
	language = {en},
	journal = {Archives of Acoustics},
	author = {Antoniuk, Pawel and Zielinski, Slawomir K},
	year = {2024},
	note = {{Accepted for publication}},
	file = {PDF:/Users/pawel/Zotero/storage/TFKC557P/Antoniuk and Zielinski - Estimating Ensemble Location and Width in Binaural Recordings of Music with Convolutional Neural Net.pdf:application/pdf},
}

@article{paul_binaural_2009,
	title = {Binaural {Recording} {Technology}: {A} {Historical} {Review} and {Possible} {Future} {Developments}},
	volume = {95},
	shorttitle = {Binaural {Recording} {Technology}},
	doi = {10.3813/AAA.918208},
	abstract = {The facsimile or true-to-original reproduction of sound events is of great interest in acoustics and related areas and has been researched for many years. One form of achieving this is binaural technology. Many consider binaural technology a very modern technology and some even consider that it is strictly related to and was invented for sound quality research. However, binaural technology, especially recording technology, has been established for some time and, in fact, the first steps were made in around 1880. Over the decades this technology has made enormous advances, due to the dedication of many people, but some challenges related to achieving a true facsimile are still to be resolved. The most important milestones and also the remaining challenges are presented herein and the prospects for the near future are discussed.},
	journal = {Acta Acustica united with Acustica},
	author = {Paul, Stephan},
	month = sep,
	year = {2009},
	pages = {767--788},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/2HZJSWFS/Paul - 2009 - Binaural Recording Technology A Historical Review and Possible Future Developments.pdf:application/pdf},
}

@article{siegfried_binaural_2003,
	title = {binaural audio in the era of virtual reality: a digest of research papers presented at recent aes conventions},
	volume = {51},
	number = {11},
	journal = {journal of the audio engineering society},
	author = {siegfried, linkwitz},
	month = nov,
	year = {2003},
	pages = {1066--1072},
}

@article{thiemann_speech_2016,
	title = {Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory scene},
	volume = {2016},
	issn = {1687-6180},
	url = {https://doi.org/10.1186/s13634-016-0314-6},
	doi = {10.1186/s13634-016-0314-6},
	abstract = {Modern binaural hearing aids utilize multimicrophone speech enhancement algorithms to enhance signals in terms of signal-to-noise ratio, but they may distort the interaural cues that allow the user to localize sources, in particular, suppressed interfering sources or background noise. In this paper, we present a novel algorithm that enhances the target signal while aiming to maintain the correct spatial rendering of both the target signal as well as the background noise. We use a bimodal approach, where a signal-to-noise ratio (SNR) estimator controls a binary decision mask, switching between the output signals of a binaural minimum variance distortionless response (MVDR) beamformer and scaled reference microphone signals. We show that the proposed selective binaural beamformer (SBB) can enhance the target signal while maintaining the overall spatial rendering of the acoustic scene.},
	number = {1},
	urldate = {2024-11-04},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Thiemann, Joachim and Müller, Menno and Marquardt, Daniel and Doclo, Simon and van de Par, Steven},
	month = feb,
	year = {2016},
	keywords = {Bilateral hearing aids, Binaural hearing aids, Binaural MVDR, Hearing aids},
	pages = {12},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/ZF9DVBTN/Thiemann et al. - 2016 - Speech enhancement for multimicrophone binaural hearing aids aiming to preserve the spatial auditory.pdf:application/pdf;Snapshot:/Users/pawel/Zotero/storage/Q3Y2KND3/s13634-016-0314-6.html:text/html},
}

@article{begault_techniques_1992,
	title = {Techniques and {Applications} for {Binaural} {Sound} {Manipulation}},
	volume = {2},
	doi = {10.1207/s15327108ijap0201_1},
	abstract = {The implementation of binaural sound to speech and auditory sound cues (auditory icons) is addressed from both applications and technical standpoints. Techniques overviewed include processing by means of filtering with head-related transfer functions. Application to advanced cockpit-human interface systems is discussed, although the techniques arc extendible to any human-machine interface. Research issues pertaining to three-dimensional sound displays under investigation at the Aerospace Human Factors Research Division at NASA-Ames Research Center are described.},
	journal = {International Journal of Aviation Psychology - INT J AVIAT PSYCHOL},
	author = {Begault, Durand and Wenzel, Elizabeth},
	month = feb,
	year = {1992},
	pages = {1--22},
	file = {Full Text PDF:/Users/pawel/Zotero/storage/K2PW3DJP/Begault and Wenzel - 1992 - Techniques and Applications for Binaural Sound Manipulation.pdf:application/pdf},
}

@article{griesinger_psychoacoustics_1997,
	title = {The {Psychoacoustics} of {Apparent} {Source} {Width}, {Spaciousness} and {Envelopment} in {Performance} {Spaces}},
	volume = {83},
	abstract = {After a review of terminology, this paper presents an hypothesis for the psychoacoustic origins of the perception of spaciousness (assumed to be closely related to or identical to the perception of envelopment) and the apparent source width (ASW). The paper proposes that the apparent source width is broadened by reflected energy arriving during the rise time of musical sound segments (notes), while the perception of envelopment arises from reflected energy which arrives during sound segments or after they end. In addition we hypothesize that the perception of the most musically desirable form of spaciousness depends on the neural process which separates sound into a foreground stream (the sound segments themselves), and a background stream (the reverberation.) Our experiments suggest that the perception of the background stream is completely inhibited for at least 50 ms after the ends of sound segments, and the inhibition decreases to zero over an additional time period of 70 ms to 120 ms. Preliminary experiments find that when there are clear gaps between foreground sound events, the loudness of the background stream is absolute - independent of the loudness of the foreground stream.This view of the psychoacoustic origin of ASW and spaciousness suggests that ASW will not depend on the absolute loudness of the foreground segments, and that when sound segments have short rise times ASW will be narrow, even in a soundfield where the perception of spaciousness (and envelopment) is strong. The proposals also imply that the perception of spaciousness (and envelopment) will depend on 1. the absolute strength of the reverberant field at least 120 to 170 ms after the ends of sound events; 2. the spatial properties of the reverberant field in the same time period; and 3. the proportion of gaps in the musical source material which allows the background stream to be heard.Although there is currently insufficient experimental data to fully support these hypotheses, they make predictions which are both useful and easily tested.},
	journal = {Acta Acustica united with Acustica},
	author = {Griesinger, David},
	month = jul,
	year = {1997},
	pages = {721--731},
	file = {PDF:/Users/pawel/Zotero/storage/W2WL5LJE/Griesinger - 1997 - The Psychoacoustics of Apparent Source Width, Spaciousness and Envelopment in Performance Spaces.pdf:application/pdf},
}

@article{antoniuk_ensemble_2024,
	title = {Ensemble width estimation in {HRTF}-convolved binaural music recordings using an auditory model and a gradient-boosted decision trees regressor},
	volume = {2024},
	copyright = {All rights reserved},
	issn = {1687-4722},
	url = {https://doi.org/10.1186/s13636-024-00374-2},
	doi = {10.1186/s13636-024-00374-2},
	abstract = {Binaural audio recordings become increasingly popular in multimedia repositories, posing new challenges in indexing, searching, and retrieval of such excerpts in terms of their spatial audio scene characteristics. This paper presents a new method for the automatic estimation of one of the most important spatial attributes of binaural recordings of music, namely “ensemble width.” The method has been developed using a repository of 23,040 binaural excerpts synthesized by convolving 192 multi-track music recordings with 30 sets of head-related transfer functions (HRTF). The synthesized excerpts represented various spatial distributions of music sound sources along a frontal semicircle in the horizontal plane. A binaural auditory model was exploited to derive the standard binaural cues from the synthesized excerpts, yielding a dataset representing interaural level and time differences, complemented by interaural cross-correlation coefficients. Subsequently, a regression method, based on gradient-boosted decision trees, was applied to the formerly calculated dataset to estimate ensemble width values. According to the obtained results, the mean absolute error of the ensemble width estimation averaged across experimental conditions amounts to 6.63° (SD 0.12°). The accuracy of the method is the highest for the recordings with ensembles narrower than 30°, yielding the mean absolute error ranging between 0.8° and 10.2°. The performance of the proposed algorithm is relatively uniform regardless of the horizontal position of an ensemble. However, its accuracy deteriorates for wider ensembles, with the error reaching 25.2° for the music ensembles spanning 90°. The developed method exhibits satisfactory generalization properties when evaluated both under music-independent and HRTF-independent conditions. The proposed method outperforms the technique based on “spatiograms” recently introduced in the literature.},
	number = {1},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Antoniuk, Paweł and Zieliński, Sławomir K. and Lee, Hyunkook},
	month = oct,
	year = {2024},
	pages = {53},
	file = {PDF:/Users/pawel/Zotero/storage/2KVL2JPM/Antoniuk et al. - 2024 - Ensemble width estimation in HRTF-convolved binaural music recordings using an auditory model and a.pdf:application/pdf},
}

@inproceedings{wabnitz_room_2010,
	title = {Room acoustics simulation for multichannel microphone arrays},
	url = {https://www.acoustics.asn.au/conference_proceedings/ICA2010/cdrom-ISRA2010/Papers/P5d.pdf},
	booktitle = {{Proceedings} of the {International} {Symposium} on {Room} {Acoustics} ({ISRA})},
	author = {Wabnitz, Andrew and Epain, Nicolas and Jin, Craig T. and Schaik, André van},
	year = {2010},
}

@book{blauert_technology_2013,
	address = {Berlin, Heidelberg},
	title = {The {Technology} of {Binaural} {Listening}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-37761-7 978-3-642-37762-4},
	url = {http://link.springer.com/10.1007/978-3-642-37762-4},
	language = {en},
	urldate = {2024-11-04},
	publisher = {Springer Berlin Heidelberg},
	editor = {Blauert, Jens},
	year = {2013},
	doi = {10.1007/978-3-642-37762-4},
	file = {Submitted Version:/Users/pawel/Zotero/storage/9YDGYC5T/Blauert - 2013 - The Technology of Binaural Listening.pdf:application/pdf},
}

@misc{decorsiere_auditory_2016,
	title = {Auditory front-end. {Two} {Ears} {Project} {Documentatio}},
	url = {https://docs.twoears.eu/en/latest/afe/},
	urldate = {2024-11-04},
	author = {Decorsière, Remi and May, Tobias},
	year = {2016},
	file = {Auditory front-end — The Two!Ears Auditory Model <unknown> documentation:/Users/pawel/Zotero/storage/4V6GPASS/afe.html:text/html},
}

@misc{raake_computational_2016,
	title = {A computational framework for modelling active exploratory listening that assigns meaning to auditory scenes—reading the world with two ears},
	url = {http://twoears.eu/},
	urldate = {2024-11-04},
	author = {Raake, Alexander},
	year = {2016},
	file = {Two!Ears:/Users/pawel/Zotero/storage/YMIAKMPD/twoears.eu.html:text/html},
}

@inproceedings{ke_lightgbm_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}
